# -*- coding: utf-8 -*-
"""videopiper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k3BP_t-v9ViYj_hEhN9ESh3Be8cd7g1U
"""

import os; os.environ["OPENAI_API_KEY"] = "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"
import os
import gradio as gr
import openai
import speech_recognition as sr
import numpy as np
import scipy.io.wavfile as wav
import traceback
import requests
import base64
import time

# Set OpenAI API key
os.environ["OPENAI_API_KEY"] = "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"
openai.api_key = os.getenv("OPENAI_API_KEY")

# Assuming 'api_key' contains your D-ID API key in the format 'username:password'
api_key = 'cnVyb3VsZXR0ZTQyMzRAZ21haWwuY29t:-zn90kEZt_X-gVsdMnWZL'  # Replace with your actual API key

# Encode the API Key
encoded_api_key = base64.b64encode(api_key.encode()).decode()

# Create the Authorization Header using the encoded API Key
auth_header = {
    'Authorization': f'Basic {encoded_api_key}'
}

# ElevenLabs API key
elevenlabs_api_key = "fdac00fd82981c34bf62ab3e402743e1"  # Replace with your ElevenLabs API key

# Conversation history and initial context
conversation_history = []
initial_context = []  # Define your initial context here if needed

# Function to update conversation history including initial context
def update_conversation_history(user_input, ai_response):
    global conversation_history
    if not conversation_history:  # Add initial context if history is empty
        conversation_history.extend(initial_context)
    conversation_history.append({"role": "user", "content": user_input})
    conversation_history.append({"role": "assistant", "content": ai_response})



# Function to transcribe speech to text
def transcribe_speech(file_path):
    recognizer = sr.Recognizer()
    with sr.AudioFile(file_path) as source:
        audio_data = recognizer.record(source)
        try:
            text = recognizer.recognize_google(audio_data)
            return text
        except sr.UnknownValueError:
            return "Speech Recognition could not understand audio"
        except sr.RequestError as e:
            return f"Could not request results from Speech Recognition service; {e}"

# Function to convert text to speech using ElevenLabs Text-to-Speech
def elevenlabs_text_to_speech(text, model_id, voice_id, api_key):
    url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
    headers = {
        "Accept": "audio/mpeg",
        "Content-Type": "application/json",
        "xi-api-key": api_key
    }
    data = {
        "text": text,
        "model_id": model_id,
        "voice_settings": {
            "stability": 0.5,
            "similarity_boost": 0.5
        }
    }

    response = requests.post(url, json=data, headers=headers)
    if response.status_code == 200:
        filename = "/tmp/response_audio.mp3"
        with open(filename, 'wb') as f:
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:
                    f.write(chunk)
        return filename
    else:
        error_msg = f"Error with TTS request: {response.status_code} - {response.text}"
        raise Exception(error_msg)


# Function to create a talk with D-ID API
def create_talk_with_did_api(text, auth_header):
    url = "https://api.d-id.com/talks"
    payload = {
        "script": {
            "type": "text",
            "input": text,
            "subtitles": "false",
            "provider": {
                "type": "elevenlabs",
                "voice_id": "XB0fDUnXU5powFXDhCwa",  # Replace with the desired ElevenLabs voice ID
                "model_id": "eleven_multilingual_v1"
            },
            "ssml": "false"
        },
        "config": {
            "fluent": "false",
            "pad_audio": "0.0",
            "stitch":True
        },
        "source_url": "https://i.ibb.co/B3BKntx/FAV-vezril-ultra-realistic-photo-of-a-stunningly-beautiful-20-year-ba48560f-9f66-4167-99c4-361b67e33.png"
    }

    # Use the provided authorization header
    headers = {
        "accept": "application/json",
        "content-type": "application/json",
        "authorization": auth_header['Authorization']
    }

    response = requests.post(url, json=payload, headers=headers)
    if response.status_code in [200, 201]:  # Check for successful response
        return response.json()['id']
    else:
        raise Exception(f"Talk creation failed: {response.status_code} - {response.text}")

# Function to get the specific talk from D-ID API
def get_specific_talk_from_did_api(talk_id, auth_header):
    response = requests.get(f'https://api.d-id.com/talks/{talk_id}', headers=auth_header)
    if response.status_code == 200:
        return response.json()['result_url']
    else:
        raise Exception(f"Failed to get talk: {response.status_code} - {response.text}")

# Function to handle the audio input and get video response
def handle_audio(input_audio):
    global conversation_history
    try:
        if input_audio is None or not isinstance(input_audio, tuple):
            raise ValueError("Invalid audio input.")

        # Extract the NumPy array (audio data) from the tuple
        audio_data = input_audio[1]
        if audio_data is None or not isinstance(audio_data, np.ndarray):
            raise ValueError("Invalid audio data.")

        # Save the NumPy array as a WAV file with a standard sampling rate (44100 Hz)
        input_audio_path = "/tmp/input_audio.wav"
        wav.write(input_audio_path, 44100, audio_data.T)

        # Transcribe the audio to text
        text = transcribe_speech(input_audio_path)

        # Prepare the GPT-3 prompt with conversation history
        full_history = conversation_history + [{"role": "user", "content": text}]
        if len(full_history) > 24:  # Limit history to prevent overload, adjust as needed
            full_history = full_history[-24:]

        # Send the text to GPT-3 using the new API format
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=full_history
        )
        gpt_response = response.choices[0].message['content']

        # Update conversation history
        update_conversation_history(text, gpt_response)

        # Convert GPT-3 response to speech using ElevenLabs Text-to-Speech
        model_id = "eleven_multilingual_v1"  # Replace with your specific model ID
        voice_id = "XB0fDUnXU5powFXDhCwa"  # The voice ID you want to use
        response_audio_path = elevenlabs_text_to_speech(gpt_response, model_id, voice_id, elevenlabs_api_key)

        # Create a talk with D-ID API
        talk_id = create_talk_with_did_api(gpt_response, auth_header)

        # Optional: Implement a delay or a polling mechanism here to wait for the video to be ready
        time.sleep(30)  # Example: wait for 30 seconds

        # Retrieve the specific talk
        video_response_url = get_specific_talk_from_did_api(talk_id, auth_header)

        return response_audio_path, "Sampling rate: 44100 Hz", input_audio_path, video_response_url
    except Exception as e:
        traceback.print_exc()
        error_message = "An error occurred: " + str(e)
        error_audio_path = elevenlabs_text_to_speech(error_message, model_id, voice_id, elevenlabs_api_key)

    # Provide a valid default response or handle the error differently
    return error_audio_path, "Sampling rate: N/A", None, "Error occurred: " + error_message

# Set up Gradio interface
iface = gr.Interface(
    fn=handle_audio,
    inputs=gr.Audio(label="Record your question to Piper"),
    outputs=[
        gr.Audio(label="Piper's Response"),
        gr.Textbox(label="Info"),
        gr.File(label="Download WAV"),
        gr.Video(label="Response Video")  # Add a video component for D-ID response
    ],
    description="Meet Piper, your personal AI assistant with a British accent! Record your question or command, and Piper will respond. Please speak clearly to help Piper understand you better."
)

iface.launch(debug=True)