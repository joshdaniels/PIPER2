{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshdaniels/PIPER2/blob/main/videopiper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a20kMcb1A5v4",
        "outputId": "f8fa7e72-1658-4a2a-8000-ee63fff52995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://94e931e9664416688b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://94e931e9664416688b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1531, in process_api\n",
            "    data = self.postprocess_data(fn_index, result[\"prediction\"], state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1421, in postprocess_data\n",
            "    outputs_cached = processing_utils.move_files_to_cache(prediction_value, block, postprocess=True)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/processing_utils.py\", line 279, in move_files_to_cache\n",
            "    return client_utils.traverse(data, _move_to_cache, client_utils.is_file_obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio_client/utils.py\", line 773, in traverse\n",
            "    return func(json_obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/processing_utils.py\", line 271, in _move_to_cache\n",
            "    temp_file_path = move_resource_to_block_cache(payload.path, block)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/processing_utils.py\", line 248, in move_resource_to_block_cache\n",
            "    return block.move_resource_to_block_cache(url_or_file_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 257, in move_resource_to_block_cache\n",
            "    temp_file_path = processing_utils.save_file_to_cache(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/processing_utils.py\", line 185, in save_file_to_cache\n",
            "    temp_dir = hash_file(file_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/processing_utils.py\", line 117, in hash_file\n",
            "    with open(file_path, \"rb\") as f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/Sampling rate: 44100 Hz'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 456, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1531, in process_api\n",
            "    data = self.postprocess_data(fn_index, result[\"prediction\"], state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1421, in postprocess_data\n",
            "    outputs_cached = processing_utils.move_files_to_cache(prediction_value, block, postprocess=True)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/processing_utils.py\", line 279, in move_files_to_cache\n",
            "    return client_utils.traverse(data, _move_to_cache, client_utils.is_file_obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio_client/utils.py\", line 773, in traverse\n",
            "    return func(json_obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/processing_utils.py\", line 271, in _move_to_cache\n",
            "    temp_file_path = move_resource_to_block_cache(payload.path, block)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/processing_utils.py\", line 248, in move_resource_to_block_cache\n",
            "    return block.move_resource_to_block_cache(url_or_file_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 257, in move_resource_to_block_cache\n",
            "    temp_file_path = processing_utils.save_file_to_cache(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/processing_utils.py\", line 185, in save_file_to_cache\n",
            "    temp_dir = hash_file(file_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/processing_utils.py\", line 117, in hash_file\n",
            "    with open(file_path, \"rb\") as f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/Sampling rate: 44100 Hz'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 501, in process_events\n",
            "    response = await self.call_prediction(awake_events, batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 465, in call_prediction\n",
            "    raise Exception(str(error) if show_error else None) from error\n",
            "Exception: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://94e931e9664416688b.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import os; os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "import os\n",
        "import gradio as gr\n",
        "import openai\n",
        "import speech_recognition as sr\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "import traceback\n",
        "import requests\n",
        "import base64\n",
        "import time\n",
        "\n",
        "# Set OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Assuming 'api_key' contains your D-ID API key in the format 'username:password'\n",
        "api_key = 'ZGRiYXNzQGdtYWlsLmNvbQ:Y6jrDw9PMU2BZ_wxFxXO4'  # Replace with your actual API key\n",
        "\n",
        "# Encode the API Key\n",
        "encoded_api_key = base64.b64encode(api_key.encode()).decode()\n",
        "\n",
        "# Create the Authorization Header using the encoded API Key\n",
        "auth_header = {\n",
        "    'Authorization': f'Basic {encoded_api_key}'\n",
        "}\n",
        "\n",
        "# ElevenLabs API key\n",
        "elevenlabs_api_key = \"fd9bb9ba5a62998f2c9c5fcc1cb5d278\"  # Replace with your ElevenLabs API key\n",
        "\n",
        "# Conversation history and initial context\n",
        "conversation_history = []\n",
        "initial_context = []  # Define your initial context here if needed\n",
        "\n",
        "# Function to update conversation history including initial context\n",
        "def update_conversation_history(user_input, ai_response):\n",
        "    global conversation_history\n",
        "    if not conversation_history:  # Add initial context if history is empty\n",
        "        conversation_history.extend(initial_context)\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": ai_response})\n",
        "\n",
        "# Function to transcribe speech to text\n",
        "def transcribe_speech(file_path):\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.AudioFile(file_path) as source:\n",
        "        audio_data = recognizer.record(source)\n",
        "        try:\n",
        "            text = recognizer.recognize_google(audio_data)\n",
        "            return text\n",
        "        except sr.UnknownValueError:\n",
        "            return \"Speech Recognition could not understand audio\"\n",
        "        except sr.RequestError as e:\n",
        "            return f\"Could not request results from Speech Recognition service; {e}\"\n",
        "\n",
        "# Function to convert text to speech using ElevenLabs Text-to-Speech\n",
        "def elevenlabs_text_to_speech(text, model_id, voice_id, api_key):\n",
        "    url = f\"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}\"\n",
        "    headers = {\n",
        "        \"Accept\": \"audio/mpeg\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"xi-api-key\": api_key\n",
        "    }\n",
        "    data = {\n",
        "        \"text\": text,\n",
        "        \"model_id\": model_id,\n",
        "        \"voice_settings\": {\n",
        "            \"stability\": 0.5,\n",
        "            \"similarity_boost\": 0.5\n",
        "        }\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=data, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        filename = \"/tmp/response_audio.mp3\"\n",
        "        with open(filename, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "        return filename\n",
        "    else:\n",
        "        error_msg = f\"Error with TTS request: {response.status_code} - {response.text}\"\n",
        "        raise Exception(error_msg)\n",
        "\n",
        "\n",
        "# Function to create a talk with D-ID API\n",
        "def create_talk_with_did_api(text, auth_header):\n",
        "    url = \"https://api.d-id.com/talks\"\n",
        "    payload = {\n",
        "        \"script\": {\n",
        "            \"type\": \"text\",\n",
        "            \"input\": text,\n",
        "            \"subtitles\": \"false\",\n",
        "            \"provider\": {\n",
        "                \"type\": \"elevenlabs\",\n",
        "                \"voice_id\": \"XB0fDUnXU5powFXDhCwa\",  # Replace with the desired ElevenLabs voice ID\n",
        "                \"model_id\": \"eleven_multilingual_v1\"\n",
        "            },\n",
        "            \"ssml\": \"false\"\n",
        "        },\n",
        "        \"config\": {\n",
        "            \"fluent\": \"false\",\n",
        "            \"pad_audio\": \"0.0\",\n",
        "            \"stitch\":True\n",
        "        },\n",
        "        \"source_url\": \"https://i.ibb.co/B3BKntx/FAV-vezril-ultra-realistic-photo-of-a-stunningly-beautiful-20-year-ba48560f-9f66-4167-99c4-361b67e33.png\"\n",
        "    }\n",
        "\n",
        "    # Use the provided authorization header\n",
        "    headers = {\n",
        "        \"accept\": \"application/json\",\n",
        "        \"content-type\": \"application/json\",\n",
        "        \"authorization\": auth_header['Authorization']\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "    if response.status_code in [200, 201]:  # Check for successful response\n",
        "        return response.json()['id']\n",
        "    else:\n",
        "        raise Exception(f\"Talk creation failed: {response.status_code} - {response.text}\")\n",
        "\n",
        "# Function to get the specific talk from D-ID API\n",
        "def get_specific_talk_from_did_api(talk_id, auth_header):\n",
        "    response = requests.get(f'https://api.d-id.com/talks/{talk_id}', headers=auth_header)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()['result_url']\n",
        "    else:\n",
        "        raise Exception(f\"Failed to get talk: {response.status_code} - {response.text}\")\n",
        "\n",
        "# Function to handle the audio input and get video response\n",
        "def handle_audio(input_audio):\n",
        "    global conversation_history\n",
        "    try:\n",
        "        if input_audio is None or not isinstance(input_audio, tuple):\n",
        "            raise ValueError(\"Invalid audio input.\")\n",
        "\n",
        "        # Extract the NumPy array (audio data) from the tuple\n",
        "        audio_data = input_audio[1]\n",
        "        if audio_data is None or not isinstance(audio_data, np.ndarray):\n",
        "            raise ValueError(\"Invalid audio data.\")\n",
        "\n",
        "        # Save the NumPy array as a WAV file with a standard sampling rate (44100 Hz)\n",
        "        input_audio_path = \"/tmp/input_audio.wav\"\n",
        "        wav.write(input_audio_path, 44100, audio_data.T)\n",
        "\n",
        "        # Transcribe the audio to text\n",
        "        text = transcribe_speech(input_audio_path)\n",
        "\n",
        "        # Prepare the GPT-3 prompt with conversation history\n",
        "        full_history = conversation_history + [{\"role\": \"user\", \"content\": text}]\n",
        "        if len(full_history) > 24:  # Limit history to prevent overload, adjust as needed\n",
        "            full_history = full_history[-24:]\n",
        "\n",
        "        # Send the text to GPT-3 using the new API format\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=full_history\n",
        "        )\n",
        "        gpt_response = response.choices[0].message['content']\n",
        "\n",
        "        # Update conversation history\n",
        "        update_conversation_history(text, gpt_response)\n",
        "\n",
        "        # Convert GPT-3 response to speech using ElevenLabs Text-to-Speech\n",
        "        model_id = \"eleven_multilingual_v1\"  # Replace with your specific model ID\n",
        "        voice_id = \"XB0fDUnXU5powFXDhCwa\"  # The voice ID you want to use\n",
        "        response_audio_path = elevenlabs_text_to_speech(gpt_response, model_id, voice_id, elevenlabs_api_key)\n",
        "\n",
        "        # Create a talk with D-ID API\n",
        "        talk_id = create_talk_with_did_api(gpt_response, auth_header)\n",
        "\n",
        "        # Optional: Implement a delay or a polling mechanism here to wait for the video to be ready\n",
        "        time.sleep(10)  # Example: wait for 10 seconds\n",
        "\n",
        "        # Retrieve the specific talk\n",
        "        video_response_url = get_specific_talk_from_did_api(talk_id, auth_header)\n",
        "\n",
        "        return response_audio_path, \"Sampling rate: 44100 Hz\", input_audio_path, video_response_url\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        error_message = \"An error occurred: \" + str(e)\n",
        "        error_audio_path = elevenlabs_text_to_speech(error_message, model_id, voice_id, elevenlabs_api_key)\n",
        "\n",
        "    # Provide a valid default response or handle the error differently\n",
        "    return error_audio_path, \"Sampling rate: N/A\", None, \"Error occurred: \" + error_message\n",
        "\n",
        "# Set up Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=handle_audio,\n",
        "    inputs=gr.Audio(label=\"Record your question to Piper\"),\n",
        "    outputs=[\n",
        "        gr.Audio(label=\"Piper's Response\"),\n",
        "        gr.Textbox(label=\"Info\"),\n",
        "        gr.File(label=\"Download WAV\"),\n",
        "        gr.Video(label=\"Response Video\")  # Add a video component for D-ID response\n",
        "    ],\n",
        "    description=\"Meet Piper, your personal AI assistant with a British accent! Record your question or command, and Piper will respond. Please speak clearly to help Piper understand you better.\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyKG89vxH/tTI2Pdr0ae/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}